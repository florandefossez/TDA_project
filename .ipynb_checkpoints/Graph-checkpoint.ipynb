{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93cdde78-b276-4675-a118-080ccdd2583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os \n",
    "from scipy.sparse import csgraph\n",
    "from scipy.linalg import eigh\n",
    "import gudhi as gd\n",
    "import gudhi.representations as tda\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow import random_uniform_initializer as rui\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow_addons  as tfa\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf0dc425-583a-4f74-819f-df1a765a303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HKS(egvec, egval, t):\n",
    "    return np.square(egvec).dot(np.diag(np.exp(-t*egval))).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "376d6f13-2e74-4901-ac5f-4f5ed152ffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extended_persistance(adj, values):\n",
    "    \n",
    "    num_vertices = adj.shape[0]\n",
    "    (xs, ys) = np.where(np.triu(adj))\n",
    "    st = gd.SimplexTree()\n",
    "    for i in range(num_vertices):\n",
    "        st.insert([i], filtration=-1e10)\n",
    "    for x, y in zip(xs, ys):        \n",
    "        st.insert([x, y], filtration=-1e10)\n",
    "    for i in range(num_vertices):\n",
    "        st.assign_filtration([i], values[i])\n",
    "    st.make_filtration_non_decreasing()\n",
    "    st.extend_filtration()\n",
    "    LD = st.extended_persistence()\n",
    "    dgmOrd0, dgmRel1, dgmExt0, dgmExt1 = LD[0], LD[1], LD[2], LD[3]\n",
    "    dgmOrd0 = np.vstack([np.array([[ min(p[1][0],p[1][1]), max(p[1][0],p[1][1]) ]]) for p in dgmOrd0 if p[0] == 0]) if len(dgmOrd0) else np.empty([0,2])\n",
    "    dgmRel1 = np.vstack([np.array([[ min(p[1][0],p[1][1]), max(p[1][0],p[1][1]) ]]) for p in dgmRel1 if p[0] == 1]) if len(dgmRel1) else np.empty([0,2])\n",
    "    dgmExt0 = np.vstack([np.array([[ min(p[1][0],p[1][1]), max(p[1][0],p[1][1]) ]]) for p in dgmExt0 if p[0] == 0]) if len(dgmExt0) else np.empty([0,2])\n",
    "    dgmExt1 = np.vstack([np.array([[ min(p[1][0],p[1][1]), max(p[1][0],p[1][1]) ]]) for p in dgmExt1 if p[0] == 1]) if len(dgmExt1) else np.empty([0,2])\n",
    "    return dgmOrd0, dgmExt0, dgmRel1, dgmExt1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9472d5b-2928-4c95-b379-61519b1d8c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'data/MUTAG/mat'\n",
    "files = os.listdir(dir)\n",
    "\n",
    "n = len(files)\n",
    "labels = np.zeros(n)\n",
    "features = []\n",
    "\n",
    "for i, file in enumerate(files):\n",
    "    data = loadmat(dir + '/' +file)\n",
    "    adj = np.array(data['A'], dtype = np.float32)\n",
    "    L = csgraph.laplacian(adj, normed = True)\n",
    "    egval, egvect = eigh(L)\n",
    "    values = HKS(egvect,egval,1)\n",
    "    dgmOrd0, dgmExt0, dgmRel1, dgmExt1 = extended_persistance(adj, values)\n",
    "    features.append(dgmOrd0)\n",
    "    features.append(dgmExt0)\n",
    "    features.append(dgmRel1)\n",
    "    features.append(dgmExt1)\n",
    "    labels[i] = int(file[file.index('lb')+3])\n",
    "    \n",
    "features = tda.DiagramScaler(use=True, scalers=[([0,1], MinMaxScaler())]).fit_transform(features)\n",
    "features = tda.Padding(use=True).fit_transform(features)\n",
    "\n",
    "features = np.array(features)\n",
    "features = np.reshape(features, (188,4,13,3))\n",
    "\n",
    "\n",
    "labels = np.array(labels, dtype = np.int32)\n",
    "\n",
    "        \n",
    "# dataset = tf.data.Dataset.from_tensor_slices(features)\n",
    "\n",
    "# rows, cols = np.where(adj == 10)\n",
    "# edges = zip(rows.tolist(), cols.tolist())\n",
    "# gr = nx.Graph()\n",
    "# gr.add_edges_from(edges)\n",
    "# nx.draw(gr, node_size=200)\n",
    "# plt.show()\n",
    "# plt.scatter(dgmOrd0[:,0], dgmOrd0[:,1])  \n",
    "# plt.scatter(dgmExt0[:,0], dgmExt0[:,1])    \n",
    "# plt.scatter(dgmRel1[:,0], dgmRel1[:,1])    \n",
    "# plt.scatter(dgmExt1[:,0], dgmExt1[:,1])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1107ea0-0798-49c4-b939-c406a70dfad4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PerslayModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(PerslayModel, self).__init__()\n",
    "\n",
    "        self.weight =  [tf.Variable(name=\"weight\", initial_value=rui(1.,1.)([10,10]), trainable = True) for _ in range(4)]\n",
    "        self.layer_vars = tf.Variable(name =\"vars\", initial_value = rui(3.,3.)([4]), trainable = True)\n",
    "        self.image_bnds = ((-0.001, 1.001), (-0.001, 1.001))\n",
    "        self.image_size = (20, 20)\n",
    "        self.fmodel = [tf.keras.Sequential([tf.keras.layers.Conv2D(10, 2, input_shape=(21,21,1)), tf.keras.layers.Flatten()]) for _ in range(4)]\n",
    "        self.rho = tf.keras.Sequential([tf.keras.layers.Dense(1, activation=\"sigmoid\", input_shape=(16000,))])\n",
    "\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        list_v = []\n",
    "        # 4 type of diagrams\n",
    "        for nf in range(4):\n",
    "            diag = inputs[:,nf,:,:]\n",
    "            tensor_diag = diag[:,:,:-1]\n",
    "            tensor_mask = diag[:,:,-1]\n",
    "            \n",
    "            # compute weights\n",
    "            grid_shape = self.weight[nf].shape  \n",
    "            indices = []\n",
    "            for dim in range(2):\n",
    "                [m, M] = (-0.001, 1.001)\n",
    "                coords = tf.slice(tensor_diag, [0, 0, dim], [-1, -1, 1])\n",
    "                ids = grid_shape[dim] * (coords - m)/(M - m)\n",
    "                indices.append(tf.cast(ids, tf.int32))\n",
    "            weight = tf.expand_dims(tf.gather_nd(params=self.weight[nf], indices=tf.concat(indices, axis=2)), -1)\n",
    "\n",
    "            # compute layer\n",
    "            num_pts = tensor_diag.shape[1]\n",
    "            coords = [tf.range(start=self.image_bnds[i][0], limit=self.image_bnds[i][1], delta=(self.image_bnds[i][1] - self.image_bnds[i][0]) / self.image_size[i]) for i in range(2)]\n",
    "            M = tf.meshgrid(*coords)\n",
    "            t = tf.concat([tf.expand_dims(tens, 0) for tens in M], axis=0)\n",
    "            bp_inp = tf.einsum(\"ijk,kl->ijl\", tensor_diag, tf.constant(np.array([[1.,-1.],[0.,1.]], dtype=np.float32)))\n",
    "            bc_inp = tf.reshape(bp_inp, [-1, num_pts, 2] + [1, 1])\n",
    "            sg = self.layer_vars[nf]\n",
    "            tensor_diag = tf.expand_dims(tf.math.exp(tf.math.reduce_sum(  -tf.math.square(bc_inp-t) / (2*tf.math.square(sg)),  axis=2)) / (2*np.pi*tf.math.square(sg)), -1)\n",
    "            \n",
    "            # Apply weight\n",
    "            output_dim = len(tensor_diag.shape) - 2\n",
    "            for _ in range(output_dim-1):\n",
    "                weight = tf.expand_dims(weight, -1)\n",
    "            tiled_weight = tf.tile(weight, [1, 1] + tensor_diag.shape[2:])\n",
    "            tensor_diag = tf.math.multiply(tensor_diag, tiled_weight)\n",
    "\n",
    "            # Apply mask\n",
    "            for _ in range(output_dim):\n",
    "                tensor_mask = tf.expand_dims(tensor_mask, -1)\n",
    "            tiled_mask = tf.tile(tensor_mask, [1, 1] + tensor_diag.shape[2:])\n",
    "            masked_layer = tf.math.multiply(tensor_diag, tiled_mask)\n",
    "\n",
    "            # Permutation invariant operation sum\n",
    "            vector = tf.math.reduce_sum(masked_layer, axis=1)\n",
    "            \n",
    "            # Convolution operation\n",
    "            vector = self.fmodel[nf](vector)\n",
    "            print('ok')\n",
    "            list_v.append(vector)\n",
    "\n",
    "        representations = tf.concat(values=list_v, axis=1)\n",
    "        final_representations = self.rho(representations)\n",
    "\n",
    "        return final_representations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0af6293e-9ae3-4f85-a26e-0866c8375872",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs    = 300\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size = 0.2, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0754b29-c6e1-43ba-b337-28bbfe1660ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PerslayModel()\n",
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.01, decay_steps=20, decay_rate=0.5, staircase=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr, epsilon=1e-4)\n",
    "optimizer = tfa.optimizers.MovingAverage(optimizer, average_decay=0.9) \n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c27608a-b61c-4b2c-85eb-6a0f0ea18a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=x_train, y=y_train, validation_data=(x_test, y_test), epochs=epochs, batch_size=32, shuffle=True)\n",
    "train_results = model.evaluate(x_train, y_train)\n",
    "test_results = model.evaluate(x_test,  y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a52868e-9662-4067-b472-017152cbe3b5",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfdad14-7340-4127-9a79-499ed3e0319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_bnds = ((-0.001, 1.001), (-0.001, 1.001))\n",
    "image_size     = (20, 20)\n",
    "\n",
    "dimension_before = 2\n",
    "coords = [tf.range(start=image_bnds[i][0], limit=image_bnds[i][1], delta=(image_bnds[i][1] - image_bnds[i][0]) / image_size[i]) for i in range(dimension_before)]\n",
    "print(coords)\n",
    "M = tf.meshgrid(*coords)\n",
    "print('------------------')\n",
    "print(len(M))\n",
    "mu = tf.concat([tf.expand_dims(tens, 0) for tens in M], axis=0)\n",
    "print('-------------------')\n",
    "print(mu.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f8561d-d96f-42da-8e51-3989122b4929",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc9cd57-8207-440f-9233-57bcc126d756",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
